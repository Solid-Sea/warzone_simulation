#!/usr/bin/env python3
"""
æˆ˜åŒºAIè®­ç»ƒæµ‹è¯•è„šæœ¬ - å®Œæ•´ç‰ˆ
åŒ…å«æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½
"""

import os
import sys
import json
import time
import numpy as np
from rl_env import WarzoneEnv, train_model, evaluate_model
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
import warnings
warnings.filterwarnings("ignore")

class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""
    def __init__(self):
        self.config = {
            "training": {
                "total_timesteps": 50000,
                "learning_rate": 1e-4,
                "n_steps": 2048,
                "batch_size": 256,
                "n_epochs": 10,
                "gamma": 0.99,
                "gae_lambda": 0.95,
                "clip_range": 0.2
            },
            "evaluation": {
                "eval_freq": 10000,
                "n_eval_episodes": 10,
                "save_freq": 25000
            },
            "environment": {
                "width": 80,
                "height": 60,
                "max_steps": 500
            }
        }
    
    def save(self, path):
        with open(path, 'w') as f:
            json.dump(self.config, f, indent=2)
    
    def load(self, path):
        if os.path.exists(path):
            with open(path, 'r') as f:
                self.config.update(json.load(f))

def create_env(config_path=None):
    """åˆ›å»ºç¯å¢ƒåŒ…è£…å™¨"""
    def _init():
        env = WarzoneEnv(config_path=config_path)
        env = Monitor(env)  # æ·»åŠ ç›‘æ§
        return env
    return _init

def setup_callbacks(model_dir, eval_env):
    """è®¾ç½®è®­ç»ƒå›è°ƒ"""
    callbacks = []
    
    # è¯„ä¼°å›è°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=os.path.join(model_dir, "best_model"),
        log_path=os.path.join(model_dir, "eval_logs"),
        eval_freq=10000,
        n_eval_episodes=10,
        deterministic=True,
        render=False
    )
    callbacks.append(eval_callback)
    
    # æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=25000,
        save_path=os.path.join(model_dir, "checkpoints"),
        name_prefix="warzone_model"
    )
    callbacks.append(checkpoint_callback)
    
    return callbacks

def run_training():
    """è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹"""
    print("ğŸ¯ å¯åŠ¨æˆ˜åŒºAIè®­ç»ƒç³»ç»Ÿ...")
    
    # åˆ›å»ºç›®å½•
    model_dir = "models"
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(os.path.join(model_dir, "checkpoints"), exist_ok=True)
    os.makedirs(os.path.join(model_dir, "eval_logs"), exist_ok=True)
    os.makedirs(os.path.join(model_dir, "logs"), exist_ok=True)
    
    # ä¿å­˜é…ç½®
    config = TrainingConfig()
    config.save(os.path.join(model_dir, "training_config.json"))
    
    # åˆ›å»ºç¯å¢ƒ
    print("ğŸŒ åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    train_env = DummyVecEnv([create_env() for _ in range(1)])
    eval_env = DummyVecEnv([create_env() for _ in range(1)])
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ¤– åˆå§‹åŒ–PPOæ¨¡å‹...")
    model = PPO(
        "MlpPolicy",
        train_env,
        learning_rate=config.config["training"]["learning_rate"],
        n_steps=config.config["training"]["n_steps"],
        batch_size=config.config["training"]["batch_size"],
        n_epochs=config.config["training"]["n_epochs"],
        gamma=config.config["training"]["gamma"],
        gae_lambda=config.config["training"]["gae_lambda"],
        clip_range=config.config["training"]["clip_range"],
        verbose=1,
        tensorboard_log=os.path.join(model_dir, "logs")
    )
    
    # è®¾ç½®å›è°ƒ
    callbacks = setup_callbacks(model_dir, eval_env)
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    
    model.learn(
        total_timesteps=config.config["training"]["total_timesteps"],
        callback=callbacks,
        progress_bar=True
    )
    
    training_time = time.time() - start_time
    print(f"âœ… è®­ç»ƒå®Œæˆ! è€—æ—¶: {training_time:.2f}ç§’")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(model_dir, "warzone_final_model")
    model.save(final_model_path)
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    return model

def quick_test():
    """å¿«é€Ÿæµ‹è¯•è®­ç»ƒæµç¨‹"""
    print("ğŸ§ª è¿è¡Œå¿«é€Ÿæµ‹è¯•...")
    
    # åˆ›å»ºç¯å¢ƒ
    env = WarzoneEnv()
    
    # æµ‹è¯•ç¯å¢ƒ
    obs, _ = env.reset()
    print(f"è§‚å¯Ÿç©ºé—´å½¢çŠ¶: {obs.shape}")
    
    # æµ‹è¯•éšæœºåŠ¨ä½œ
    for step in range(5):
        action = env.action_space.sample()
        obs, reward, terminated, truncated, info = env.step(action)
        print(f"æ­¥éª¤ {step+1}: åŠ¨ä½œ={action}, å¥–åŠ±={reward:.2f}, çº¢æ–¹={info['red_units']}, è“æ–¹={info['blue_units']}")
        
        if terminated or truncated:
            break
    
    print("âœ… å¿«é€Ÿæµ‹è¯•å®Œæˆ")

def load_and_evaluate(model_path=None):
    """åŠ è½½å¹¶è¯„ä¼°æ¨¡å‹"""
    if model_path is None:
        model_path = "models/warzone_final_model"
    
    if not os.path.exists(f"{model_path}.zip"):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return
    
    print("ğŸ“Š åŠ è½½å¹¶è¯„ä¼°æ¨¡å‹...")
    
    # åŠ è½½æ¨¡å‹
    model = PPO.load(model_path)
    
    # è¯„ä¼°
    evaluate_model(model, episodes=20)
    
    print("âœ… è¯„ä¼°å®Œæˆ")

def benchmark_training():
    """è®­ç»ƒæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("âš¡ è¿è¡Œè®­ç»ƒæ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    # æµ‹è¯•ä¸åŒé…ç½®
    configs = [
        {"total_timesteps": 1000, "learning_rate": 1e-3},
        {"total_timesteps": 5000, "learning_rate": 1e-4},
        {"total_timesteps": 10000, "learning_rate": 1e-4}
    ]
    
    results = []
    
    for i, cfg in enumerate(configs):
        print(f"\næµ‹è¯•é…ç½® {i+1}: {cfg}")
        
        start_time = time.time()
        model = train_model(**cfg)
        training_time = time.time() - start_time
        
        # å¿«é€Ÿè¯„ä¼°
        env = WarzoneEnv()
        obs, _ = env.reset()
        total_reward = 0
        
        for _ in range(100):
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            
            if terminated or truncated:
                break
        
        result = {
            "config": cfg,
            "training_time": training_time,
            "final_reward": float(total_reward)
        }
        results.append(result)
        
        print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f}s, æœ€ç»ˆå¥–åŠ±: {total_reward:.2f}")
    
    # ä¿å­˜ç»“æœ
    def convert_to_serializable(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.float32):
            return float(obj)
        elif isinstance(obj, np.int32):
            return int(obj)
        return obj
    
    with open("benchmark_results.json", 'w') as f:
        json.dump(results, f, indent=2, default=convert_to_serializable)
    
    print("âœ… åŸºå‡†æµ‹è¯•å®Œæˆï¼Œç»“æœå·²ä¿å­˜")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æˆ˜åŒºAIè®­ç»ƒç³»ç»Ÿ")
    parser.add_argument("--mode", choices=["train", "test", "evaluate", "benchmark"], 
                        default="test", help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--model-path", type=str, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--timesteps", type=int, default=50000, help="è®­ç»ƒæ­¥æ•°")
    
    args = parser.parse_args()
    
    if args.mode == "train":
        config = TrainingConfig()
        config.config["training"]["total_timesteps"] = args.timesteps
        run_training()
    elif args.mode == "test":
        quick_test()
    elif args.mode == "evaluate":
        load_and_evaluate(args.model_path)
    elif args.mode == "benchmark":
        benchmark_training()

if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œè¿è¡Œå¿«é€Ÿæµ‹è¯•
    if len(sys.argv) == 1:
        quick_test()
    else:
        main()
