"""
é«˜æ€§èƒ½æˆ˜åŒºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒ v2.1 - ä¼˜åŒ–ç‰ˆ
ä¿®å¤å¾ªç¯ä¾èµ–ï¼Œå¢å¼ºåœ°å½¢å½±å“ï¼Œä¼˜åŒ–å¥–åŠ±å‡½æ•°
"""
import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
import os
from typing import Dict, List, Tuple, Any, Optional
import json

class Config:
    """é…ç½®ç®¡ç†ç±» - æ¶ˆé™¤ç¡¬ç¼–ç """
    def __init__(self, config_path: Optional[str] = None):
        self.config = {
            "env": {
                "width": 80,
                "height": 60,
                "max_steps": 500,
                "obstacle_ratio": 0.1
            },
            "units": {
                "infantry": {"health": 100, "attack_range": 3, "damage": 25, "speed": 2},
                "tank": {"health": 150, "attack_range": 4, "damage": 40, "speed": 1},
                "artillery": {"health": 80, "attack_range": 6, "damage": 60, "speed": 1}
            },
            "rewards": {
                "move": 0.01,
                "attack": 0.5,
                "kill": 10.0,
                "defense": 0.05,
                "terrain_bonus": 0.1,
                "formation_bonus": 0.2
            },
            "terrain": {
                "obstacle_penalty": -0.1,
                "cover_bonus": 0.2,
                "high_ground_bonus": 0.3
            }
        }
        
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r') as f:
                self.config.update(json.load(f))
    
    def get(self, key: str, default=None):
        keys = key.split('.')
        value = self.config
        for k in keys:
            value = value.get(k, default)
        return value

class WarzoneEnv(gym.Env):
    """ä¼˜åŒ–ç‰ˆæˆ˜åŒºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒ"""
    
    def __init__(self, width: int = None, height: int = None, config_path: Optional[str] = None):
        super(WarzoneEnv, self).__init__()
        
        self.config = Config(config_path)
        self.width = width or self.config.get("env.width")
        self.height = height or self.config.get("env.height")
        self.max_steps = self.config.get("env.max_steps")
        
        # åŠ¨ä½œç©ºé—´æ‰©å±•ï¼š6ä¸ªåŸºç¡€åŠ¨ä½œ + 2ä¸ªæˆ˜æœ¯åŠ¨ä½œ
        self.action_space = spaces.Discrete(8)
        
        # ä¼˜åŒ–è§‚å¯Ÿç©ºé—´ï¼šåŒ…å«åœ°å½¢ä¿¡æ¯
        self.observation_space = spaces.Box(
            low=0, high=1, 
            shape=(7 * self.height * self.width,),  # å¢åŠ åœ°å½¢é€šé“
            dtype=np.float32
        )
        
        self.reset()
    
    def reset(self, seed=None, **kwargs):
        """é‡ç½®ç¯å¢ƒ - æ”¯æŒéšæœºç§å­"""
        super().reset(seed=seed)
        
        self.step_count = 0
        self.units = self._initialize_units()
        self.terrain = self._initialize_terrain()
        self.terrain_features = self._calculate_terrain_features()
        
        return self._get_observation(), {}
    
    def _initialize_units(self) -> Dict[int, Dict]:
        """åˆå§‹åŒ–å•ä½ - ä½¿ç”¨é…ç½®ç³»ç»Ÿ"""
        units = {}
        unit_id = 1
        
        # çº¢æ–¹å•ä½é…ç½®
        red_positions = [(10 + i % 5, 10 + i // 5) for i in range(13)]
        unit_types = ["infantry"] * 10 + ["tank"] * 2 + ["artillery"] * 1
        
        for unit_type, pos in zip(unit_types, red_positions):
            units[unit_id] = self._create_unit(unit_type, "red", pos)
            unit_id += 1
        
        # è“æ–¹å•ä½é…ç½®
        blue_positions = [(60 + i % 5, 50 + i // 5) for i in range(13)]
        for unit_type, pos in zip(unit_types, blue_positions):
            units[unit_id] = self._create_unit(unit_type, "blue", pos)
            unit_id += 1
            
        return units
    
    def _create_unit(self, unit_type: str, camp: str, position: Tuple[int, int]) -> Dict:
        """åˆ›å»ºå•ä½å®ä¾‹ - ä½¿ç”¨é…ç½®"""
        config = self.config.get(f"units.{unit_type}")
        return {
            "type": unit_type,
            "position": position,
            "camp": camp,
            "health": config["health"],
            "max_health": config["health"],
            "attack_range": config["attack_range"],
            "damage": config["damage"],
            "speed": config["speed"]
        }
    
    def _initialize_terrain(self) -> np.ndarray:
        """åˆå§‹åŒ–åœ°å½¢ - æ™ºèƒ½éšœç¢ç‰©åˆ†å¸ƒ"""
        terrain = np.zeros((self.height, self.width), dtype=np.float32)
        
        # æ·»åŠ éšœç¢ç‰©é›†ç¾¤è€Œééšæœºåˆ†å¸ƒ
        num_clusters = int(self.width * self.height * self.config.get("env.obstacle_ratio") / 25)
        
        for _ in range(num_clusters):
            cluster_x = np.random.randint(10, self.width - 10)
            cluster_y = np.random.randint(10, self.height - 10)
            cluster_size = np.random.randint(3, 8)
            
            for dx in range(-cluster_size, cluster_size + 1):
                for dy in range(-cluster_size, cluster_size + 1):
                    x, y = cluster_x + dx, cluster_y + dy
                    if 0 <= x < self.width and 0 <= y < self.height:
                        if np.random.random() < 0.7:  # 70%æ¦‚ç‡æ”¾ç½®éšœç¢ç‰©
                            terrain[y, x] = 1
        
        return terrain
    
    def _calculate_terrain_features(self) -> Dict[str, np.ndarray]:
        """è®¡ç®—åœ°å½¢ç‰¹å¾ï¼šé«˜åœ°ã€æ©ä½“ç­‰"""
        features = {
            "high_ground": np.zeros((self.height, self.width), dtype=np.float32),
            "cover": np.zeros((self.height, self.width), dtype=np.float32)
        }
        
        # é«˜åœ°ï¼šè¾¹ç¼˜åŒºåŸŸ
        for y in range(self.height):
            for x in range(self.width):
                if self.terrain[y, x] == 0:  # ééšœç¢ç‰©
                    # é«˜åœ°ï¼šé è¿‘åœ°å›¾è¾¹ç¼˜
                    distance_to_edge = min(x, y, self.width - 1 - x, self.height - 1 - y)
                    if distance_to_edge <= 5:
                        features["high_ground"][y, x] = 1.0
                    
                    # æ©ä½“ï¼šé è¿‘éšœç¢ç‰©
                    nearby_obstacles = 0
                    for dy in range(-2, 3):
                        for dx in range(-2, 3):
                            nx, ny = x + dx, y + dy
                            if 0 <= nx < self.width and 0 <= ny < self.height:
                                if self.terrain[ny, nx] == 1:
                                    nearby_obstacles += 1
                    
                    features["cover"][y, x] = min(nearby_obstacles / 8.0, 1.0)
        
        return features
    
    def _get_observation(self) -> np.ndarray:
        """è·å–è§‚å¯ŸçŠ¶æ€ - åŒ…å«åœ°å½¢ç‰¹å¾"""
        obs = np.zeros((7, self.height, self.width), dtype=np.float32)
        
        # é€šé“0-1: çº¢è“æ–¹å•ä½ä½ç½®
        # é€šé“2-3: çº¢è“æ–¹å•ä½è¡€é‡ï¼ˆæ ‡å‡†åŒ–ï¼‰
        # é€šé“4: åœ°å½¢éšœç¢
        # é€šé“5: é«˜åœ°ä¼˜åŠ¿
        # é€šé“6: æ©ä½“è¦†ç›–
        
        for unit in self.units.values():
            x, y = unit["position"]
            if 0 <= x < self.width and 0 <= y < self.height:
                channel = 0 if unit["camp"] == "red" else 1
                health_channel = 2 if unit["camp"] == "red" else 3
                
                obs[channel, y, x] = 1.0
                obs[health_channel, y, x] = unit["health"] / unit["max_health"]
        
        obs[4] = self.terrain
        obs[5] = self.terrain_features["high_ground"]
        obs[6] = self.terrain_features["cover"]
        
        return obs.flatten()
    
    def _calculate_terrain_bonus(self, position: Tuple[int, int], camp: str) -> float:
        """è®¡ç®—åœ°å½¢å¥–åŠ±åŠ æˆ"""
        x, y = position
        if not (0 <= x < self.width and 0 <= y < self.height):
            return 0.0
        
        bonus = 0.0
        
        # é«˜åœ°å¥–åŠ±
        if self.terrain_features["high_ground"][y, x] > 0:
            bonus += self.config.get("terrain.high_ground_bonus")
        
        # æ©ä½“å¥–åŠ±
        cover_value = self.terrain_features["cover"][y, x]
        bonus += cover_value * self.config.get("terrain.cover_bonus")
        
        return bonus
    
    def _calculate_formation_bonus(self, unit: Dict) -> float:
        """è®¡ç®—é˜µå‹å¥–åŠ±ï¼ˆå•ä½ååŒï¼‰"""
        x, y = unit["position"]
        friendly_nearby = 0
        
        for other in self.units.values():
            if other["camp"] == unit["camp"] and other != unit:
                dist = abs(x - other["position"][0]) + abs(y - other["position"][1])
                if dist <= 3:  # 3æ ¼èŒƒå›´å†…çš„å‹å†›
                    friendly_nearby += 1
        
        return min(friendly_nearby * self.config.get("rewards.formation_bonus"), 1.0)
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:
        """æ‰§è¡Œä¸€æ­¥ç¯å¢ƒæ›´æ–° - ä¼˜åŒ–å¥–åŠ±è®¡ç®—"""
        self.step_count += 1
        
        # æ‰§è¡ŒåŠ¨ä½œ
        reward = self._execute_action(action)
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        terminated = self._check_termination()
        truncated = self.step_count >= self.max_steps
        
        # è·å–æ–°è§‚å¯Ÿ
        obs = self._get_observation()
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯
        info = {
            "step": self.step_count,
            "red_units": sum(1 for u in self.units.values() if u["camp"] == "red"),
            "blue_units": sum(1 for u in self.units.values() if u["camp"] == "blue")
        }
        
        return obs, reward, terminated, truncated, info
    
    def _execute_action(self, action: int) -> float:
        """æ‰§è¡ŒåŠ¨ä½œå¹¶è®¡ç®—å¥–åŠ± - åŒ…å«åœ°å½¢å’Œé˜µå‹å½±å“"""
        reward = 0.0
        
        # æ‰©å±•åŠ¨ä½œç©ºé—´ï¼š0-3ç§»åŠ¨ï¼Œ4-5æ”»å‡»ï¼Œ6-7æˆ˜æœ¯åŠ¨ä½œ
        if action < 4:  # ç§»åŠ¨åŠ¨ä½œ
            reward += self._execute_move_action(action)
        elif action < 6:  # æ”»å‡»åŠ¨ä½œ
            reward += self._execute_attack_action(action - 4)
        else:  # æˆ˜æœ¯åŠ¨ä½œ
            reward += self._execute_tactical_action(action - 6)
        
        return reward
    
    def _execute_move_action(self, direction: int) -> float:
        """æ‰§è¡Œç§»åŠ¨åŠ¨ä½œ"""
        dx, dy = [(0, -1), (0, 1), (-1, 0), (1, 0)][direction]
        total_reward = 0.0
        
        for unit in self.units.values():
            if unit["camp"] == "red":
                new_x = max(0, min(self.width-1, unit["position"][0] + dx * unit["speed"]))
                new_y = max(0, min(self.height-1, unit["position"][1] + dy * unit["speed"]))
                
                # æ£€æŸ¥åœ°å½¢éšœç¢
                if self.terrain[new_y, new_x] == 0:
                    unit["position"] = (new_x, new_y)
                    
                    # åŸºç¡€ç§»åŠ¨å¥–åŠ±
                    total_reward += self.config.get("rewards.move")
                    
                    # åœ°å½¢å¥–åŠ±
                    total_reward += self._calculate_terrain_bonus((new_x, new_y), "red")
                    
                    # é˜µå‹å¥–åŠ±
                    total_reward += self._calculate_formation_bonus(unit)
                else:
                    # éšœç¢ç‰©æƒ©ç½š
                    total_reward += self.config.get("terrain.obstacle_penalty")
        
        return total_reward
    
    def _execute_attack_action(self, attack_type: int) -> float:
        """æ‰§è¡Œæ”»å‡»åŠ¨ä½œ - åŒºåˆ†è¿‘æˆ˜å’Œè¿œç¨‹"""
        reward = 0.0
        
        for red_unit in self.units.values():
            if red_unit["camp"] != "red" or red_unit["health"] <= 0:
                continue
                
            for blue_id, blue_unit in list(self.units.items()):
                if blue_unit["camp"] != "blue" or blue_unit["health"] <= 0:
                    continue
                
                dist = abs(red_unit["position"][0] - blue_unit["position"][0]) + \
                       abs(red_unit["position"][1] - blue_unit["position"][1])
                
                # æ ¹æ®æ”»å‡»ç±»å‹è°ƒæ•´æœ‰æ•ˆèŒƒå›´
                effective_range = red_unit["attack_range"] * (1.2 if attack_type == 1 else 1.0)
                
                if dist <= effective_range:
                    # è®¡ç®—ä¼¤å®³ï¼ˆè€ƒè™‘åœ°å½¢ï¼‰
                    damage = red_unit["damage"]
                    
                    # é˜²å®ˆæ–¹æ©ä½“å‡ä¼¤
                    cover_bonus = self.terrain_features["cover"][blue_unit["position"][1], 
                                                               blue_unit["position"][0]]
                    damage *= (1 - cover_bonus * 0.3)
                    
                    blue_unit["health"] -= int(damage)
                    reward += self.config.get("rewards.attack")
                    
                    if blue_unit["health"] <= 0:
                        reward += self.config.get("rewards.kill")
                        del self.units[blue_id]
        
        return reward
    
    def _execute_tactical_action(self, tactic_type: int) -> float:
        """æ‰§è¡Œæˆ˜æœ¯åŠ¨ä½œï¼š0=é˜²å¾¡å§¿æ€ï¼Œ1=é›†ç»“"""
        reward = 0.0
        
        if tactic_type == 0:  # é˜²å¾¡å§¿æ€
            for unit in self.units.values():
                if unit["camp"] == "red":
                    terrain_bonus = self._calculate_terrain_bonus(unit["position"], "red")
                    reward += terrain_bonus * 2  # é˜²å¾¡æ—¶åœ°å½¢å¥–åŠ±ç¿»å€
        
        else:  # é›†ç»“ - å‘å‹å†›é æ‹¢
            for unit in self.units.values():
                if unit["camp"] == "red":
                    formation_bonus = self._calculate_formation_bonus(unit)
                    reward += formation_bonus
        
        return reward
    
    def _check_termination(self) -> bool:
        """æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶ - ä¼˜åŒ–åˆ¤æ–­"""
        red_alive = sum(1 for u in self.units.values() if u["camp"] == "red" and u["health"] > 0)
        blue_alive = sum(1 for u in self.units.values() if u["camp"] == "blue" and u["health"] > 0)
        
        return red_alive == 0 or blue_alive == 0

def train_model(total_timesteps=100000, learning_rate=1e-4, config_path=None):
    """è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ¨¡å‹ - ä¼˜åŒ–ç‰ˆ"""
    print("ğŸš€ åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆå¼ºåŒ–å­¦ä¹ ç¯å¢ƒ...")
    env = WarzoneEnv(config_path=config_path)
    
    print("âš™ï¸  é…ç½®PPOæ¨¡å‹å‚æ•°...")
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=2048,
        batch_size=256,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        verbose=1,
        tensorboard_log="./logs/",
        device="auto"
    )
    
    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒ (å…± {total_timesteps} æ­¥)...")
    model.learn(total_timesteps=total_timesteps)
    
    # ä¿å­˜æ¨¡å‹å’Œé…ç½®
    os.makedirs("models", exist_ok=True)
    model_path = f"models/warzone_ppo_v2"
    model.save(model_path)
    
    # ä¿å­˜é…ç½®
    with open(f"{model_path}_config.json", 'w') as f:
        json.dump(env.config.config, f, indent=2)
    
    print(f"âœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
    return model

def evaluate_model(model, episodes=10, render=False):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹ - å¢å¼ºç‰ˆ"""
    print("ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    env = WarzoneEnv()
    total_rewards = []
    wins = 0
